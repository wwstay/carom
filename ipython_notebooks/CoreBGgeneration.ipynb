{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cPickle as pk\n",
    "import collections\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reduce series of lists down to a single list\n",
    "def flatten2one(xS):\n",
    "    return [item for sub_list in xS.tolist() for item in sub_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate initial uniform distribution\n",
    "def initial_probs(lis_names):\n",
    "    vocab_ctr = collections.Counter((flatten2one(lis_names)))\n",
    "    prob = collections.OrderedDict()\n",
    "    networds = len(flatten2one(lis_names))\n",
    "    for k,v in vocab_ctr.items():\n",
    "        if isinstance(k,float):\n",
    "            continue\n",
    "        prob[k]=v/networds\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate z(word)\n",
    "def word_estimation(name,C,B):\n",
    "    z=dict()\n",
    "    #t[\"name\"] = name\n",
    "    #t[\"B_prob\"]= [B[word] for word in name]\n",
    "    denom=0.0\n",
    "    fake_news = []\n",
    "    for w in name:\n",
    "        try:\n",
    "            denom+=C[w]/B[w]\n",
    "        except ZeroDivisionError:\n",
    "            z[w] = 1\n",
    "            fake_news.append(w)\n",
    "    \n",
    "    for word in name:\n",
    "        if word in fake_news:\n",
    "            continue\n",
    "        #t[\"last\"] = word\n",
    "        #t[\"denom\"] = denom\n",
    "        try:\n",
    "            z[word] = (C[word]/B[word])/denom\n",
    "        except ZeroDivisionError:\n",
    "            z[word]=1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#After getting new z(w), update the parameters/distributions C and B to better represent the data\n",
    "def update_weights(Z,C,B,networds):\n",
    "    c=dict()\n",
    "    b=dict()\n",
    "    for word in C:\n",
    "        #print word\n",
    "        new_c = 0.0\n",
    "        new_b = 0.0\n",
    "        for z in Z:\n",
    "            #print z,\n",
    "            if word in z:\n",
    "                #print z[word]\n",
    "                new_c+=z[word]\n",
    "                new_b+=(1-z[word])\n",
    "    \n",
    "        new_c=new_c/len(Z)\n",
    "        new_b=new_b/(networds-len(Z))\n",
    "        #print \"new C({}): \".format(word), new_c\n",
    "        c[word] = new_c\n",
    "        b[word] = new_b\n",
    "        #print \"next\"\n",
    "    \n",
    "    #print c\n",
    "    return (c,b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the EM algorithm that updates the distribution until some degree of convergence\n",
    "def core_algo1(names):\n",
    "    #initialize probabilities\n",
    "    prob_C = prob_B = initial_probs(names)\n",
    "    networds = len(flatten2one(names))\n",
    "    for i in range(len(prob_C)):\n",
    "        #pdb.set_trace()\n",
    "        all_z = names.apply(word_estimation,args=(prob_C,prob_B))\n",
    "        prob_C,prob_B = update_weights(all_z,prob_C,prob_B,networds)\n",
    "    return prob_C,prob_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns probabilities of being core or background\n",
    "def assrtcore(word,pofcore):\n",
    "    return pofcore[word]\n",
    "\n",
    "def assrtbg(word,pofcore):\n",
    "    return (1.000001-pofcore[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate edit distance, if (1-edit distance) or how many characters do we retain>75% of the length,\n",
    "#then we probably have a misspelling, and we can assert that both are the same.\n",
    "def distmetric(s1,s2,pr1,pr2):\n",
    "    workons1 = set(s1) - set(s2)\n",
    "    workons2 = set(s2) - set(s1)\n",
    "    #Misspellings would be part of the set outside the union\n",
    "    ret = 0.0\n",
    "    for w1 in workons1:\n",
    "        #filter out those words from sentence 2 that are 75% close to this particular word w1 from sentence 1 \n",
    "        dat = filter(lambda x: (1-editdistance.eval(w1,x)/len(x))>0.75, workons2)\n",
    "        if len(dat)<1:\n",
    "            continue\n",
    "        #The misspelled words are the same, thus both are part of core or part of background\n",
    "        ret+= reduce( (lambda x,y: x*y),[pr2[a] for a in dat])*pr1[w1] + reduce((lambda x,y: (1-x)*(1-y)),[pr2[a] for a in dat])*(1-pr1[w1])\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extra edit functions that can factor into the probability\n",
    "def edit_func(s1,s2,pr1,pr2):\n",
    "    fn=0.0\n",
    "    \n",
    "    #abbreviation probability 2 word abbrvs and 3 word abbrvs\n",
    "    dat =( abbrv_func(s1,s2,pr1,pr2,2)+abbrv_func(s1,s2,pr1,pr2,3) )\n",
    "    fn+=dat\n",
    "    \n",
    "    #Probability from misspelling\n",
    "    fn = fn+distmetric(s1,s2,pr1,pr2)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If I abbreviate Cafe Coffee Day to CCD, they both are still the same, this creates abbreviations and check if same\n",
    "def abbrv_func(s1,s2,pr1,pr2,n):\n",
    "    #n corresponds to number of words abbreviations\n",
    "    ret = 0.0\n",
    "    for i in range(len(s1)-n+1):\n",
    "        w1=s1[i:i+n]\n",
    "        abbs = ''.join([w[:1] for w in w1])\n",
    "        #If abbreviation exists then those are same, and thus we can say that these words and \n",
    "        #their abbreviations are all part of the core or all part of background\n",
    "        if(abbs in s2):\n",
    "            ret+= reduce( (lambda x,y: x*y),[pr1[a] for a in w1])*pr2[abbs] + reduce((lambda x,y: (1-x)*(1-y)),[pr1[a] for a in w1])*(1-pr2[abbs])\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The core deduplication algorithm\n",
    "def dedup(sent1,sent2,pofcore1,pofcore2):\n",
    "    #To be a same match, all core words should be the same and all other words should be background\n",
    "    fprob = 0.0\n",
    "    common_w = set(sent1).intersection(sent2)\n",
    "    s1_uncommon = set(sent1) - set(sent2)\n",
    "    s2_uncommon = set(sent2) - set(sent1)\n",
    "    commonprob = 1.0\n",
    "    #If a word is common, it should be part of the core in both sentences or part of background in both sentences\n",
    "    for w_ in common_w:\n",
    "        commonprob*=(assrtcore(w_,pofcore1)*assrtcore(w_,pofcore2) + assrtbg(w_,pofcore1)*assrtbg(w_,pofcore2))\n",
    "    \n",
    "    #print \"Commons: \", common_w, commonprob\n",
    "    #If a word is in sentence 1 but not in sentence 2, then it should be a background word\n",
    "    prob1 = 1.0\n",
    "    for w1_ in s1_uncommon:\n",
    "        prob1*=assrtbg(w1_,pofcore1)\n",
    "    \n",
    "    #print \"uncommon1: \", s1_uncommon, prob1\n",
    "    \n",
    "    #If a word is in sentence 2 but not in sentence 1, then it should be a background\n",
    "    prob2 = 1.0\n",
    "    for w2_ in s2_uncommon:\n",
    "        prob2*=assrtbg(w2_,pofcore2)\n",
    "    \n",
    "    #print \"uncommon2: \", s2_uncommon, prob2\n",
    "    \n",
    "    fprob = fprob + prob1*prob2*commonprob\n",
    "    #print \"fprob\", fprob\n",
    "    \n",
    "    #Add probabilities gotten from edit functions\n",
    "    fprob= fprob+edit_func(sent1,sent2,pofcore1,pofcore2)\n",
    "    \n",
    "    return fprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A test function to test my dedup algorithm\n",
    "def test_sample(alpha,pC,pB):\n",
    "    #sample_set = pd.Series([[\"best\", \"western\", \"lamplighter\", \"inn\"],[\"best\",\"western\"],[\"Hotel\",\"Amsterdam\"]])\n",
    "    #pC,pB = core_algo1(sample_set)\n",
    "    \n",
    "    pofcore = dict()\n",
    "    for k,v in pC.items():\n",
    "        print k,v\n",
    "        pofcore[k] = alpha*pC[k]/(alpha*pC[k] + (1-alpha)*pB[k])\n",
    "    p1 =[\"best\", \"western\",\"hotel\"]\n",
    "    p2= [\"best\",\"western\"]\n",
    "    #print pofcore\n",
    "    print dedup(p1,p2,pofcore,pofcore)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coreprob(pC,pB, alpha):\n",
    "    pofcore = dict()\n",
    "    for k,v in pC.items():\n",
    "        pofcore[k] = alpha*pC[k]/(alpha*pC[k] + (1-alpha)*pB[k])\n",
    "    return pofcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the saved features from prepare_4_EM\n",
    "def loaddistributions(filename):\n",
    "    emfeatures = pk.load(open(filename+\".pk\",\"rb\"))\n",
    "    alpha = 0.45\n",
    "    #Working with only london properties as of now\n",
    "    lon_names = emfeatures.name\n",
    "    lon_addr = emfeatures.address\n",
    "\n",
    "    #Bring out the Core and Background distributions from the EM algorithm of name\n",
    "    pC_distr_name,pB_distr_name = core_algo1(lon_names)\n",
    "    pC_distr_addr, pB_distr_addr = core_algo1(lon_addr)\n",
    "    \n",
    "    #Generate probability of being a core\n",
    "    pcore_name = coreprob(pC_distr_name,pB_distr_name,alpha)\n",
    "    pcore_addr = coreprob(pC_distr_addr,pB_distr_addr,alpha)\n",
    "    \n",
    "    pk.dump((pcore_name,pcore_addr),open(filename+\"distributions.pk\",\"wb\"))\n",
    "    print \"File: \", filename+\"distributions.pk created\"\n",
    "    return (pcor_name,pcore_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 9.75 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        a=i*j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EXTRA:\n",
    "\n",
    "# def preclean(dat):\n",
    "#     cln_d = set(flatten2one(dat))\n",
    "    \n",
    "#     def snip(x,cln):\n",
    "#         if len(x)<1:\n",
    "#             return\n",
    "#         for idx,w in enumerate(x):\n",
    "#             if len(w)<=2 or w.isdigit():\n",
    "#                 continue\n",
    "#             if w[:-1] in cln:\n",
    "#                 x[idx]=w[:-1]\n",
    "\n",
    "#     dat.apply(snip,args=(cln_d,))\n",
    "#     return dat\n",
    "\n",
    "#Extra function not used, part of old experiments\n",
    "# def corecmp(w1,w2, pofcore):\n",
    "#     if(len(w1)==0 or len(w2)==0):\n",
    "#         return 0\n",
    "#     if w1[0]==w2[0]:\n",
    "#         return assrtcore(w1[0], pofcore)*assrtcore(w2[0],pofcore)\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# def abbrv(w1,w2, pofcore1,pofcore2):\n",
    "#     if(len(w1)==0 or len(w2)==0):\n",
    "#         return 0\n",
    "#     abbs = ''.join([w[:1] for w in w1])\n",
    "#     print \"inside abbrv: \", abbs\n",
    "#     if abbs==w2[0]:\n",
    "#         return assrtcore(w1,pofcore1)*assrtcore(w2,pofcore2) + assrtbg(w1,pofcore1)*assrtbg(w2,pofcore2)\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# def dedup(p1,p2,i,j,pofcore):\n",
    "#     if(i==len(p1) and j==len(p2)):\n",
    "#         return 1\n",
    "#     ret = 0\n",
    "\n",
    "#     #PI = {corecmp: assrtcore, abbrv: assrtcore, bginsert: assrtbg, bgdelete: assrtbg}\n",
    "#     E = {corecmp: (1,1), abbrv: (2,1), bginsert: (1,1), bgdelete: (1,2)}\n",
    "#     for e in E.keys():\n",
    "#         l,r = E[e] \n",
    "#         W1 = p1[i:i+l]\n",
    "#         W2 = p2[j:j+r]\n",
    "#         pi = e(W1,W2,pofcore)\n",
    "#         if pi==0:\n",
    "#             continue\n",
    "#         ret+=pi*dedup(p1,p2,i+l,j+r)\n",
    "    \n",
    "#     return ret\n",
    "\n",
    "\n",
    "\n",
    "#lon_names = preclean(lon_names)\n",
    "#lon_addr = preclean(lon_addr)\n",
    "#set(flatten2one(lon_names))\n",
    "\n",
    "# #s1 = [\"cafe\",\"coffee\",\"day\"]\n",
    "# #s2= [\"cafe\",\"coffee\",\"day\"]\n",
    "# #s2 = [\"cafe\",\"coffee\",\"day\"]\n",
    "# s1=[\"best\",\"western\",\"hotel\"]\n",
    "# s2=[\"bw\",\"hotel\"]\n",
    "# tempp = {\"cafe\": 0.5,\"coffee\": 0.45,\"day\": 1.0, \"ccd\": 1.0, \"best\":0.3,\"western\": 0.5,\"bw\":0.70,\"hotel\":0.24,\"coffefe\":0.45,\"gay\":0.8}\n",
    "# print dedup(s1,s2,tempp,tempp)\n",
    "\n",
    "#exmpl = pd.Series([[\"starbucks\",\"coffee\"],[\"kalmane\",\"coffee\"],[\"starbucks\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
