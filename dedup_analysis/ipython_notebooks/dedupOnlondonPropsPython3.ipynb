{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"CoreBGgeneration.ipynb\"\n",
    "%run \"prepare_4_EM.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are all the functions you will use, they are commented and tell you what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_wide_dedup(cityparam1,cityparam2,city_name,supplier_name):\n",
    "    \"\"\"\n",
    "    The algorithm that is being run in parallel and matching properties city by city\n",
    "\n",
    "    Args:\n",
    "        cityparam1 (DataFrame): The subset of properties from table 1 of a particular city\n",
    "        cityparam2 (DataFrame): The subset of properties from table 2 of a particular city\n",
    "        city_name (str): Name of the city we are subsetting\n",
    "        supplier_name (List): Names of the supplier\n",
    "    Returns:\n",
    "        DataFrame: A dataframe same as cityparam1 with predicted matches appended in a new column called \"final_mapped\"\n",
    "\n",
    "    \"\"\"\n",
    "    pname1,paddr1 = city_wide_distr(cityparam1,0.8,city_name+supplier_name[0])\n",
    "    pname2,paddr2 = city_wide_distr(cityparam2,0.8,city_name+supplier_name[1])\n",
    "    \n",
    "    vector_dict_Pcorename = makecumulativedict(pname1,pname2)\n",
    "    vector_dict_Pcoreaddr = makecumulativedict(paddr1,paddr2)\n",
    "    \n",
    "    cityparam1[\"final_mapped\"]= cityparam1.apply(matchrecord,args=(pname1,paddr1,\n",
    "                                              cityparam2,\n",
    "                                              pname2,paddr2,vector_dict_Pcorename,\n",
    "                                              vector_dict_Pcoreaddr),\n",
    "                                              axis=1)\n",
    "    return cityparam1\n",
    "def city_wide_distr(params,alpha,filename,save=True):    \n",
    "    \"\"\"\n",
    "    Creates required probability distribution of names and addresses\n",
    "\n",
    "    Args:\n",
    "        params (DataFrame): The dataframe of cleaned and tokenized properties with names and addresses\n",
    "        alpha (float): The weight of core words. If alpha is 1, all words are regarded as core words and if alpha is 0 all words are regarded as background\n",
    "        filename (str): Name to save distributions to pickle file. Would work if save is True.\n",
    "        save (bool): If save is True, it will save the distribution to pickle file.\n",
    "\n",
    "    Returns:\n",
    "        pair: (core probability of words in hotel names, core probability of words in hotel addresses) \n",
    "    \"\"\"\n",
    "    #Working with only london properties as of now\n",
    "    names = params.name\n",
    "    addr = params.address\n",
    "\n",
    "    try:\n",
    "        (pcore_name,pcore_addr) = pk.load(open(filename+\"distributions.pk\",'rb'))\n",
    "    except:   \n",
    "        #Bring out the Core and Background distributions from the EM algorithm of name\n",
    "        pC_distr_name,pB_distr_name = core_algo1(names)\n",
    "        pC_distr_addr, pB_distr_addr = core_algo1(addr)\n",
    "\n",
    "        #Generate probability of being a core\n",
    "        pcore_name = coreprob(pC_distr_name,pB_distr_name,alpha)\n",
    "        pcore_addr = coreprob(pC_distr_addr,pB_distr_addr,alpha)\n",
    "    \n",
    "        if save:\n",
    "            pk.dump((pcore_name,pcore_addr),open(filename+\"distributions.pk\",\"wb\"))\n",
    "            print(\"File: \", filename+\"distributions.pk created\")\n",
    "    \n",
    "    return pcore_name,pcore_addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makecumulativedict(p_corename1,p_corename2):\n",
    "    \"\"\"\n",
    "    This function appends different word:prob dictionary for each table to a single dict with all words.\n",
    "    (so the new dict has word:[prob1,prob2] for words common in both tables). This function is useful to produce vectors of probabilities (running gen_probvec) \n",
    "    \n",
    "    Args:\n",
    "        p_corename1 (dict): Dictionary of word:probability from table 1\n",
    "        p_corename2 (dict): Dictionary of word:probability from table 2\n",
    "\n",
    "    Returns:\n",
    "        OrderedDict: A combined dictionary of word:list(probs) of both tables\n",
    "    \"\"\"\n",
    "    vector_dict = OrderedDict([(k,[]) for k in sorted(p_corename1)])\n",
    "    vector_dict = add_vals_to_vectdic(vector_dict,p_corename1)\n",
    "    vector_dict = add_vals_to_vectdic(vector_dict,p_corename2)\n",
    "    return vector_dict\n",
    "\n",
    "def add_vals_to_vectdic(odict,addict):\n",
    "    \"\"\"\n",
    "    This function creates a dictionary that contains the probabilistic values of words of both the tables.\n",
    "    So a key would be the word and the value would be a list (of max 2 elements when comparing 2 and if one word doesn't exist in both tables then only 1)\n",
    "\n",
    "    Args:\n",
    "        odict (dict): An ordered dictionary passed in with vals as list and keys as words\n",
    "        addict (dict): The probabilities of each words you want to append to the ordered dictionary\n",
    "\n",
    "    Returns:\n",
    "        dict: A ordered dictionary where the keys are words and values are lists of probabilities from different tables\n",
    "\n",
    "    \"\"\"\n",
    "    for key,val in addict.items():  #python3 has changed iteritems to items\n",
    "        if key in odict:\n",
    "            odict[key].append(val)\n",
    "        else:\n",
    "            odict[key] = []\n",
    "            odict[key].append(val)\n",
    "            \n",
    "    return odict\n",
    "\n",
    "def gen_probvec(name,pCdict,i):\n",
    "    \"\"\"\n",
    "    Generates a vector of length number of words (each words corresponds to an index). For each hotel name/address the vector this function generates is:\n",
    "    The core probability of word in each idx corresponding to words of the hotel name/address.\n",
    "    (So if our total table was- \"Hotel A\" and \"B\" the vector would be of dim 3 and for \"Hotel A\" the vector would be [p(hotel),p(A),0] )\n",
    "    \n",
    "    Args:\n",
    "        name (list): A list of tokenized words\n",
    "        pCdict (dict): A dictionary containing the probability of word being core as a list for both tables\n",
    "        i (int): index number indicating whether it comes from table 1 or 2\n",
    "\n",
    "    Return:\n",
    "        list: A sparse vector of num_words dimension with probabilities. \n",
    "    \"\"\"\n",
    "    vec = np.zeros(len(pCdict))\n",
    "    for word in name:\n",
    "        idx = list(pCdict.keys()).index(word)\n",
    "        if len(pCdict[word])==1:\n",
    "            vec[idx] = pCdict[word][0]\n",
    "        else:\n",
    "            vec[idx] = pCdict[word][i]\n",
    "    \n",
    "    return vec\n",
    "    \n",
    "def genreg_vec(name,pCdict):\n",
    "    \"\"\"\n",
    "    Generates a vector of length number of words (each words corresponds to an index). For each hotel name/address the vector this function generates is:\n",
    "    For each word in a particular name/address, the vector has a 1 and for the rest 0.\n",
    "    So if our total table was - \"hotel A\",\"b\" then the vector would be 3 dimensional and for \"hotel A\" this function would spit out would be [1,1,0]\n",
    "\n",
    "    Args:\n",
    "        name (list): A list of tokenized words\n",
    "        pCdict (dict): A dictionary containing the probability of a word being core (key=word, val=probability)\n",
    "    Returns:\n",
    "        list: A one hot vector of num_of_words dimension \n",
    "    \"\"\"\n",
    "    vec=np.zeros(len(pCdict))\n",
    "    for word in name:\n",
    "        idx = list(pCdict.keys()).index(word) #python3 change from pCdict.keys().index to what is seen here because dict keys needs to be converted to list now\n",
    "        vec[idx]=1.0\n",
    "        \n",
    "    return vec\n",
    "def check_result(inp1,inp2,idcolumn):\n",
    "    \"\"\"\n",
    "    A function used to check accuracy, and extract values that were predicted wrong.\n",
    "\n",
    "    Args:\n",
    "        inp1 (DataFrame): Matched dataframe according to model\n",
    "        inp2 (DataFrame): The original dataframe to which matching occured with human matched values\n",
    "        idcolumn (str): The column name where the algorithm outputted the matches\n",
    "    Returns:\n",
    "        triplet: Three DataFrames that include the merged dataframe after prediction, the ones that were wrong and the third dataframe with the wrong outputs merged with table 2.\n",
    "\n",
    "    \"\"\"\n",
    "    result = pd.merge(inp1,inp2,left_on=[idcolumn],right_on=[\"mapped_to_ext_id\"])\n",
    "    falses = result[result[idcolumn]!=result[\"mapped_to_ext_id_x\"]]\n",
    "    print(\"Acc: \", 1.0*len(result[result[idcolumn]==result[\"mapped_to_ext_id_x\"]])/len(result))\n",
    "    print(\"num falses: \", len(falses))\n",
    "    print(\"num total: \", len(result))\n",
    "    wrongones = pd.merge(falses[[\"name_x\",\"address_x\",\"mapped_to_ext_id_x\",idcolumn]],inp2[[\"name\",\"address\",\"mapped_to_ext_id\"]],left_on=[\"mapped_to_ext_id_x\"],right_on=[\"mapped_to_ext_id\"])\n",
    "    return result,falses,wrongones\n",
    "\n",
    "def vect_matchmaker(row, vect_name,vect_addr, data2):\n",
    "    \"\"\"\n",
    "    A wrapper to apply cossine similarity measure for each row in dataframe2. This function is used as an apply function.\n",
    "\n",
    "    Args:\n",
    "        row (Series): A row of a dataframe1\n",
    "        vect_name (dict): An ordered dictionary of words of names from both DataFrames\n",
    "        vect_addr (dict):  An ordered dictionary of words of addresses from both DataFrames\n",
    "        data2 (DataFrame): The whole second dataframe to match with\n",
    "    Returns:\n",
    "        Series: A series of cossine similarities of row with every single entry in data2 \n",
    "\n",
    "    \"\"\"\n",
    "    measure = data2.apply(get_cossim,args=(row,vect_name,vect_addr),axis=1)\n",
    "    return measure\n",
    "\n",
    "def get_cossim(row2,row1,vect_name,vect_addr):\n",
    "    \"\"\"\n",
    "    Gets the cossine similarity between two vectors. It concatenates vectors of name and address into a larger dimensional vector and computes the cossine similarity\n",
    "    This function is used as an apply function.\n",
    "\n",
    "    Args:\n",
    "        row2 (Series): A row of a dataframe2\n",
    "        row1 (Series): A row of a dataframe1\n",
    "        vect_name (dict): The cumulative vector dictionary made from makecumulativedict for names\n",
    "        vect_addr (dict): The cumulative vector dictionary made from makecumulativedict for addresses\n",
    "    \n",
    "    Returns:\n",
    "        float: The similarity measure\n",
    "\n",
    "    \"\"\"\n",
    "    genvec2 = np.concatenate([genreg_vec(row2.loc[\"name\"],vect_name), genreg_vec(row2.loc[\"address\"],vect_addr)])\n",
    "    genvec1 = np.concatenate([genreg_vec(row1.loc[\"name\"],vect_name), genreg_vec(row1.loc[\"address\"],vect_addr)])\n",
    "\n",
    "    #genvec2 = genreg_vec(row2.loc[\"address\"],vect_addr)\n",
    "    #genvec1 = genreg_vec(row1.loc[\"address\"],vect_addr)\n",
    "    sim_measure_gen = 1-spatial.distance.cosine(genvec1,genvec2)\n",
    "    if math.isnan(sim_measure_gen):\n",
    "                sim_measure_gen = 0.0\n",
    "            \n",
    "    return sim_measure_gen\n",
    "\n",
    "def haverdist(row2,row1):\n",
    "    \"\"\"\n",
    "    A function that computes haversine distance between two pairs of latitudes and longitudes\n",
    "    Used as an apply function\n",
    "    \n",
    "    Args:\n",
    "        row2 (Series): A row of a dataframe2\n",
    "        row1 (Series): A row of a dataframe1\n",
    "\n",
    "    Returns:\n",
    "        float: 1-haversine_distance between entry in row1 and entry in row2. So 1 indicates on top of each other, while 0 indicates very far.\n",
    "    \n",
    "    \"\"\"\n",
    "    if pd.isnull(row1.latitude) | pd.isnull(row2.latitude) | pd.isnull(row1.longitude) | pd.isnull(row2.longitude):\n",
    "        return 0.0\n",
    "        \n",
    "    lat1 = math.radians(row1.latitude)\n",
    "    lon1 = math.radians(row1.longitude)\n",
    "    lat2 = math.radians(row2.latitude)\n",
    "    lon2 = math.radians(row2.longitude)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    return (1-c)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distprobs(row,data2):\n",
    "    \"\"\"\n",
    "    A wrapper to apply haversine distance metric to entries between a row in dataframe1 to every entry in dataframe2\n",
    "\n",
    "    Args:\n",
    "        row (Series): A row of a dataframe1\n",
    "        data2 (DataFrame): The dataframe to match dataframe1 with.\n",
    "\n",
    "    Returns:\n",
    "        Series: A series of haveresine closeness for entry in row 1 with every entry in data2\n",
    "    \"\"\"\n",
    "    dists = data2.apply(haverdist,args=(row,),axis=1)\n",
    "    return dists\n",
    "\n",
    "\n",
    "\n",
    "def matching(row2,row1,pCname_self,pCaddr_self, pCsent_name, pCsent_addr):\n",
    "    \"\"\"\n",
    "    This function runs the dedup algorithm from CoreBGgeneration for hotel names and addresses.\n",
    "\n",
    "    Args:\n",
    "        row2 (Series): A row of a dataframe2\n",
    "        row1 (Series): A row of a dataframe1\n",
    "        pCname_self: The word core probabilities for hotel names in dataframe2\n",
    "        pCaddr_self: The word core probabilities for hotel addresses in dataframe2\n",
    "        pCsent_name: The word core probabilities for hotel names in dataframe1\n",
    "        pCsent_addr: The word core probabilities for hotel addresses in dataframe1\n",
    "\n",
    "    Returns:\n",
    "        float: The probability of being a match.\n",
    "    \"\"\"\n",
    "    name_prob = dedup(row1.loc[\"name\"],row2.loc[\"name\"],pCsent_name,pCname_self)\n",
    "    addr_prob = dedup(row1.loc[\"address\"],row2.loc[\"address\"],pCsent_addr,pCaddr_self)\n",
    "    #print \"name:address probability, \", name_prob, address_prob\n",
    "    return name_prob*(addr_prob+0.2*name_prob)\n",
    "    \n",
    "\n",
    "def matchrecord(row,pCname_self,pCaddr_self,data2,pCname2,pCaddr2,vect_name,vect_addr):\n",
    "    \"\"\"\n",
    "    This function combines all different models and methods into one and provides the best match as result.\n",
    "    .. note:: This needs to improve in terms of how much weight/how much should we listen to which model. This is a common ensemble method problem, and should be solved with the training set provided.\n",
    "    \n",
    "    Args:\n",
    "        row (Series): A row of a dataframe1\n",
    "        pCname_self (dict): The word core probabilities for hotel names in dataframe1\n",
    "        pCaddr_self (dict): The word core probabilities for hotel addresses in dataframe1\n",
    "        data2 (DataFrame): The dataframe to match with\n",
    "        pCname2 (dict): The word core probabilities for hotel names in dataframe2\n",
    "        pCaddr2 (dict): The word core probabilities for hotel addresses in dataframe2\n",
    "        vect_name (dict): An ordered dictionary of words of names from both DataFrames\n",
    "        vect_addr (dict): An ordered dictionary of words of addresses from both DataFrames\n",
    "\n",
    "    Returns:\n",
    "        str: The index with the maximum probability of being a match, or \"none\" if none found\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    probs=data2.apply(matching,args=(row,pCname2,pCaddr2,pCname_self,pCaddr_self),axis=1)\n",
    "    \n",
    "    \n",
    "    vect_prob = vect_matchmaker(row,vect_name,vect_addr,data2)\n",
    "\n",
    "    latlongprob = distprobs(row,data2)\n",
    "    \n",
    "    #fnalprobs = 0.14*probs + 0.86*vect_prob + latlongprob\n",
    "    fnalprobs = vect_prob + latlongprob\n",
    "\n",
    "    if (probs + vect_prob).max()<=1e-04:\n",
    "        return \"none\"\n",
    "    \n",
    "    return data2.loc[fnalprobs.idxmax(),\"mapped_to_ext_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribution(prop1,prop2,supplier_name1,supplier_name2,city_name):\n",
    "    \"\"\"\n",
    "    This function is the heart. Running this runs your whold deduplication between supplier_name1 and supplier_name2.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        prop1 (DataFrame): DataFrame object for property 1 we are using to compare\n",
    "        prop2 (DataFrame): DataFrame object for property 2 we are using to compare\n",
    "        supplier_name1 (str): Name of table where we got prop1 properties from \n",
    "        supplier_name2 (str): Name of table where we got prop2 properties from\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The overall matched resolved dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    ccolsaddr1 = get_relevcols(prop1,\"address\")\n",
    "    ccolsname1 = get_relevcols(prop1,\"name\")\n",
    "    \n",
    "    ccolsaddr2 = get_relevcols(prop2,\"address\")\n",
    "    ccolsname2 = get_relevcols(prop2,\"name\")\n",
    "\n",
    "    def appendercols(prop,ccols,colname):\n",
    "        val = dict()\n",
    "        for c in ccols:\n",
    "            val[c] = \"\"\n",
    "        prop.fillna(value=val)\n",
    "        prop[colname] = \"\"\n",
    "        for c in ccols:\n",
    "            prop[colname]+=prop[c]+\" \"\n",
    "            del prop[c]\n",
    "            \n",
    "    if len(ccolsaddr1)>1:\n",
    "        appendercols(prop1,ccolsaddr1,\"address\")\n",
    "    if len(ccolsname1)>1:\n",
    "        appendercols(prop1,ccolsname1,\"name\")\n",
    "        \n",
    "    if len(ccolsaddr2)>1:\n",
    "        appendercols(prop2,ccolsaddr2, \"address\")\n",
    "    if len(ccolsname2)>1:\n",
    "        appendercols(prop2,ccolsname2, \"name\")\n",
    "    \n",
    "    try:\n",
    "        params1 = pk.load(open(\"emfeatures_\"+supplier_name1+\"_\"+city_name+\".pk\",\"rb\"))\n",
    "    except:\n",
    "        params1 = prepareem(prop1,supplier_name1)\n",
    "    try:\n",
    "        params2 = pk.load(open(\"emfeatures_\"+supplier_name2+\"_\"+city_name+\".pk\",\"rb\"))\n",
    "    except:\n",
    "        params2 = prepareem(prop2,supplier_name2)\n",
    "    \n",
    "    citynames1 = params1.city.unique().tolist()\n",
    "    \n",
    "    \n",
    "    citynames2 = params2.city.unique().tolist()\n",
    "    cities = set(citynames1).intersection(set(citynames2))\n",
    "\n",
    "    result = []\n",
    "    supp_names = [supplier_name1,supplier_name2]\n",
    "    for i,c in enumerate(cities):\n",
    "        p1 = params1[hotelbeds_props.city==c]\n",
    "        p2 = params2[taap_props.city==c]\n",
    "        result.append(city_wide_dedup(p1,p2,c,supp_names))\n",
    "\n",
    "    return result\n",
    "def df_gen(dat):\n",
    "    dat.columns = map(str.lower,dat.columns)\n",
    "    dat = dat[pd.notnull(dat.mapped_to_ext_id)]\n",
    "    dat[[\"latitude\",\"longitude\"]] = dat[[\"latitude\",\"longitude\"]].replace(0.0,np.nan)\n",
    "\n",
    "    return dat \n",
    "def appendercols(prop,ccols,colname):\n",
    "    val = dict()\n",
    "    for c in ccols:\n",
    "        val[c] = \"\"\n",
    "    prop.fillna(value=val)\n",
    "    prop[colname] = \"\"\n",
    "    for c in ccols:\n",
    "        prop[colname]+=prop[c]+\" \"\n",
    "        del prop[c]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is how to run the code to get the results you want. First we shall generate the code by running the mongo client and picking out the necessary city. In the example below I have used London. prop_* are all dataframes with city london. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "prop_lon_taap = pd.DataFrame(list(client['supplier_static_database'].taap_properties.find({\"City\": \"London\", \"Country\": \"GBR\"}))) \n",
    "prop_lon_idb = pd.DataFrame(list(client['inventorydb'].property.find({\"city_name\": \"London\", \"country\": 243})))\n",
    "prop_lon_taap.columns = map(str.lower,prop_lon_taap.columns) #before it was unicode.lower -- python3 has apparantly updated unicode to str, and the old str to bytes\n",
    "prop_lon_idb.columns = map(str.lower, prop_lon_idb.columns) #before it was unicode.lower\n",
    "prop_lon_taap = prop_lon_taap[~pd.isnull(prop_lon_taap.mapped_to_ext_id)]\n",
    "prop_lon_idb = prop_lon_idb[~pd.isnull(prop_lon_idb.ext_id)]\n",
    "\n",
    "prop_lon_idb[\"mapped_to_ext_id\"] = prop_lon_idb.ext_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_lon_idb.replace(\"\",np.nan,inplace=True)\n",
    "prop_lon_idb[[\"latitude\",\"longitude\"]] = prop_lon_idb[[\"latitude\",\"longitude\"]].replace(0.0,np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idb_params = prepareem(prop_lon_idb,\"inventorydb_LON\")\n",
    "prop_subdf = prop_lon_idb\n",
    "stop_words,stemingdic = loadfilters()\n",
    "\n",
    "#Clean data and tokenize\n",
    "namedat=  prop_subdf[get_relevcols(prop_subdf,'name')].iloc[:,0].str.lower()\n",
    "addrdat = prop_subdf[get_relevcols(prop_subdf,'address')].iloc[:,0].str.lower()\n",
    "#namedat = prop_lon_idb.name\n",
    "#addrdat = prop_lon_idb.address\n",
    "\n",
    "namedat = namedat.apply(gramclean, args=(stop_words,))\n",
    "addrdat = addrdat.apply(gramclean, args=(stop_words,))\n",
    "\n",
    "#Transliterate non ASCII characters\n",
    "namedat = namedat.apply(translittunicode)\n",
    "addrdat = addrdat.apply(translittunicode)\n",
    "\n",
    "#Stem certain common words\n",
    "namedat = namedat.apply(stemmer, args=(stemingdic,))\n",
    "addrdat = addrdat.apply(stemmer, args=(stemingdic,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preclean function generates the emfeatures that get stored in the pickle file showed above. Here as an example I have pre-cleaned and generated the emfeatures for london properties in inventory_database. This process usually takes some time so there is benefit to speeding this up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             [66, knightsbridge]\n",
       "1       [1525, hogarth, rd, earl, ct, kensington]\n",
       "2                 [50, lancaster, gatehyde, park]\n",
       "3               [100, queen, gtwy, s, kensington]\n",
       "4              [60, hyde, park, gtwy, kensington]\n",
       "                          ...                    \n",
       "5729                            [12, norwich, st]\n",
       "5730                  [fl, 4b, 153, cromwell, rd]\n",
       "5731                          [191, cromwell, rd]\n",
       "5732                    [fl, 3739, arkwright, rd]\n",
       "5733                 [fl, gdn, 142, cromwell, rd]\n",
       "Name: address, Length: 5734, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emfeatures = pd.DataFrame({\"name\": namedat,\n",
    "                \"address\": addrdat,\n",
    "                \"city\": prop_lon_idb['city_name'].str.lower(),\n",
    "                \"latitude\":prop_lon_idb['latitude'],\n",
    "                \"longitude\":prop_lon_idb['longitude'],\n",
    "                \"mapped_to_ext_id\":prop_lon_idb[\"ext_id\"]\n",
    "                              })\n",
    "\n",
    "\n",
    "emfeatures.city = emfeatures.city.apply(translittunicode)\n",
    "preclean(emfeatures.name)\n",
    "preclean(emfeatures.address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dump the tokenized and stemmed data into a pickle\n",
    "pk.dump(emfeatures,open(\"emfeatures_\"+\"inventorydb_LON_with_get_relev_cols\"+\".pk\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an example of opening these tokenized features from a generated pickle file.\n",
    "with open(\"../pickles/emfeatures_taap_LON.pk\",'rb') as f:\n",
    "    taap_props =  pk.load(f, encoding='latin1')\n",
    "\n",
    "with open(\"./emfeatures_inventorydb_LON_with_get_relev_cols.pk\",'rb') as f:\n",
    "    idb_params =  pk.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the portion of the code that matches and does entitiy resolution on the two dataframes provided. This could be speeded up as well. city_wide_dedup function takes care of the deduplication and returns a deduplicated dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idb_params= emfeatures\n",
    "citynames1 = idb_params.city.unique().tolist()\n",
    "citynames2 = taap_props.city.unique().tolist()\n",
    "cities = set(citynames1).intersection(set(citynames2))\n",
    "result = []\n",
    "supp_names = [\"inventory_db\",\"taap\"]\n",
    "for i,c in enumerate(cities):\n",
    "    p1 = idb_params[idb_params.city==c]\n",
    "    p2 = taap_props[taap_props.city==c]\n",
    "    result.append(city_wide_dedup(p1,p2,c,supp_names))\n",
    "\n",
    "#Store the result into CSV\n",
    "result[0].to_csv(\"taap_inventorydb_LON_dedup.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact sheikh@wwstay.com "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
